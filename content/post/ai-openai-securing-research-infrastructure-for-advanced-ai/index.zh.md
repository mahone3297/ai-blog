+++
title = '[AI OpenAI] 保护前沿AI研究基础设施的安全'
date = 2024-06-06T09:41:45+08:00
draft = false
categories = ['AI', 'OpenAI']
tags = ['AI', 'OpenAI']
description = '概述支持OpenAI前沿AI模型安全训练的架构。'
keywords = ['AI', 'OpenAI', '安全AI训练', '研究基础设施', 'AI安全']
+++

我们概述了支持前沿模型安全训练的架构。

![Media > Security](https://images.ctfassets.net/kftzwdyauwt9/3ND00DMJCGocsrnNtx1E15/4037d25fe74733aaf604061293faf647/Alignment_Blog_5_29.png?w=1920&q=90&fm=webp)

我们分享了一些关于我们研究超级计算机安全架构的高级细节。

OpenAI运营着一些最大规模的AI训练超级计算机，使我们能够提供在能力和安全性方面均处于行业领先地位的模型，同时推进AI前沿的发展。我们的使命是确保先进AI惠及每个人，而这一工作的基础是支持我们研究的基础设施。

为了安全地实现这一使命，我们优先考虑这些系统的安全性。这里，我们概述了当前支持大规模前沿模型安全训练的架构和操作。这包括设计用于在安全环境中保护敏感模型权重的措施以促进AI创新。尽管这些安全功能将随着时间的推移而发展，但我们认为提供当前对我们研究基础设施安全性的看法是有价值的。我们希望这些见解能帮助其他AI研究实验室和安全专业人员在保障他们自己的系统时（我们也在招聘）。

## 威胁模型
研究基础设施由于实验工作负载的多样性和快速演变性，呈现出独特的安全挑战。

研究基础设施是几个重要资产的家园，这些资产必须得到保护。其中，未发布的模型权重是最重要的保护对象，因为它们代表核心知识产权，需要防止未经授权的泄露或破坏。

出于这个目的，OpenAI创建了一系列专门用于前沿模型开发和安全的研究环境。研究基础设施必须支持保护模型权重、算法秘密和其他用于开发前沿模型的敏感资产，防止其被未经授权的外泄和破坏。同时，研究人员必须有足够的资源和基础计算设施访问权限，以确保他们的生产力和效率。

## 架构
我们的研究技术架构建立在Azure上，利用Kubernetes进行编排。我们利用这两者来实现一个符合我们的威胁模型的安全架构。

### 1. 身份基础

我们的身份基础建立在Azure Entra ID（以前的Azure Active Directory）上。Azure Entra ID与内部认证和授权框架及控制集成。Azure Entra ID在会话创建时启用基于风险的验证、使用身份验证令牌以及检测异常登录。这些功能补充了我们内部的检测工具，用于识别和阻止潜在威胁。

### 2. Kubernetes架构

我们使用Kubernetes在我们的基础设施中编排和管理工作负载。研究工作负载受到Kubernetes角色基于访问控制（RBAC）政策的保护，以遵循最小权限原则。准入控制器政策为工作负载设定了安全基线，控制容器权限和网络访问以降低风险。

我们依赖现代VPN技术为我们的研究环境提供安全网络连接。网络政策定义了工作负载如何与外部服务通信。我们采用默认拒绝出口策略并明确列出授权的外部通信路径。我们广泛使用私有链接网络路由，减少必需的互联网路由，并使这个白名单尽可能简短。

对于一些高风险任务，我们使用gVisor（一个容器运行时）提供额外的隔离。这种多层次的防御方法确保了工作负载的强大安全性和高效管理。

### 3. 存储敏感数据

像凭证、秘密和服务账户等敏感数据需要额外保护。我们使用密钥管理服务在我们的研究基础设施中存储和管理敏感信息，并通过角色基于访问控制限制访问，使只有授权的工作负载和用户才能检索或修改这些信息。

### 4. 研究人员和开发人员的身份和访问管理（IAM）

访问管理对于管理上述系统的研究人员和开发人员访问至关重要。任何IAM解决方案的安全目标是跨资源实现时限的“最小权限”访问策略、高效管理和可审计性。

为此，我们构建了一个名为AccessManager的服务，作为管理内部授权和实现最小权限授权的可扩展机制。该服务将访问管理决策联邦化给根据政策定义的审批者。这确保了对敏感资源（包括模型权重）的访问授权决策由具有适当监督的授权人员做出。

AccessManager政策可以定义得严格或灵活，具体取决于相关资源。请求和被授予访问敏感资源（如包含模型权重的研究环境存储）需要多方批准。对于敏感资源，AccessManager授权在指定时间后到期，这意味着如果不续期，权限将恢复为无权限状态。通过实施这些控制，我们降低了未经授权的内部访问和员工账户被破坏的风险。

我们将GPT-4集成到AccessManager中，以促进最小权限角色分配。用户可以在AccessManager中搜索资源，服务将使用我们的模型建议可授予该资源访问权限的角色。将用户连接到更具体的角色有助于减少对广泛、通用和权限过大的角色的依赖。在初始角色请求和多方批准步骤中（如果指定角色的政策要求），人的介入降低了模型建议错误角色的风险。

### 5. CI/CD安全

我们的基础设施团队使用持续集成和持续交付（CI/CD）管道来构建和测试我们的研究基础设施。我们在保障我们的基础设施CI/CD管道方面进行了投资，使其更具抗潜在威胁的能力，同时保持我们的研究人员和工程师的开发和部署过程的完整性和速度。

我们限制创建、访问和触发与基础设施相关的管道的能力，以防止访问CI/CD服务可用的秘密。CI/CD工作人员的访问也同样受到限制。合并代码到部署分支需要多方批准，增加了额外的监督和安全层。我们使用基础设施即代码（IaC）范式，以一致、可重复和安全的方式配置大规模基础设施。CI在我们基础设施的每次变更中多次强制执行预期配置。

### 6. 灵活性

同时，研究需要推动前沿。这可能需要对我们的基础设施进行快速迭代，以支持不断变化的功能需求和限制。这种灵活性对于实现安全和功能需求至关重要，在某些情况下，为实现这些目标，允许适当的补偿控制的例外是至关重要的。

## 保护模型权重
从研究环境中防止模型权重被外泄需要多层次的安全方法。这些定制控制旨在保护我们的研究资产免受未经授权的访问和盗窃，同时确保它们在研究和开发过程中保持可访问性。这些措施可能包括：

- 授权：对包含敏感模型权重的研究存储账户的访问授权需要多方批准。
- 访问：研究模型权重的存储资源通过私有链接接入OpenAI的环境，以减少对互联网的暴露，并通过Azure进行身份验证和授权以进行访问。
- 出口控制：OpenAI的研究环境使用网络控制，只允许出口流量到特定预定义的互联网目标。对未列入白名单的主机的网络流量将被拒绝。
- 检测：OpenAI维护了一套综合的检测控制措施来支撑这一架构。出于安全原因，这些控制的详细信息不予公开。

## 审计和测试
OpenAI使用内部和外部红队模拟对手测试我们的研究环境安全控制。我们已邀请领先的第三方安全咨询公司对我们的研究环境进行了渗透测试，而我们的内部红队对我们的优先事项进行了深度评估。

我们正在探索研究环境的合规制度。由于保护模型权重是一个定制的安全问题，建立覆盖这一挑战的合规框架需要进行一些定制。目前，我们正在评估现有的安全标准和特定于保护AI技术的自定义控制。这可能会扩展到包括针对AI系统独特安全挑战的AI特定安全和监管标准，如Cloud Security Alliance的AI安全倡议或NIST SP 800-218 AI更新的最新努力。

## 未来控制的研究与开发
保护日益先进的AI系统需要持续的创新和适应。我们在开发新的安全控制方面处于前沿，如我们在“重新构想先进AI的安全基础设施”博客文章中所述。我们对研究和开发的承诺确保我们始终领先于新兴威胁，并继续增强我们的AI基础设施的安全性。

## 加入我们
在OpenAI，我们致力于不断发展和保护先进的AI。我们邀请AI和安全社区加入我们的使命。通过申请我们的网络安全资助计划或加入我们的团队，您的贡献可以帮助塑造AI安全的未来。

### 开放职位：

- 软件工程师，安全
- 安全工程师，检测与响应（美国、英国、日本）
- 企业安全工程师
- 研究工程师，隐私

---

<!-- - [原文](...) -->
- [original](https://openai.com/index/securing-research-infrastructure-for-advanced-ai/)
<!-- - [博客 - 从零开始学AI](...) -->
<!-- - [Blog | Learn AI from scratch](...) -->
<!-- - [公众号 - 从零开始学AI](...) -->
<!-- - [CSDN - 从零开始学AI](...) -->
<!-- - [掘金 - 从零开始学AI](...) -->
<!-- - [知乎 - 从零开始学AI](...) -->
<!-- - [阿里云 - 从零开始学AI](...) -->
<!-- - [腾讯云 - 从零开始学AI](...) -->
