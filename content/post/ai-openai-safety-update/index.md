+++
title = '[AI OpenAI] OpenAI 安全更新'
date = 2024-05-27T09:58:44+08:00
draft = false
categories = ['AI', 'OpenAI-blog']
tags = ['AI', 'OpenAI']
description = "AI 首尔峰会中分享我们的实践。我们自豪地构建并发布了在能力和安全性方面都处于行业领先地位的模型。超过一亿用户和数百万开发者依赖于我们安全团队的工作。"
keywords = ["OpenAI", "AI", "安全更新", "人工智能", "首尔峰会", "模型安全", "AI安全", "AI发展", "安全措施", "安全投资", "安全框架", "安全测试", "人工智能风险", "AI安全承诺", "安全研究", "模型行为", "滥用监控", "儿童保护", "选举公正", "政策分析", "安全决策", "董事会监督"]
+++

AI 首尔峰会中分享我们的实践

![safety-blog-cover-02](https://images.ctfassets.net/kftzwdyauwt9/54pUOkZ0poSpo9udfZmN3g/e90a4235b3d8537bdddfcc7219b636d1/safety-blog-cover-02.jpg?w=1920&q=90&fm=webp)

我们自豪地构建并发布了在能力和安全性方面都处于行业领先地位的模型。

超过一亿用户和数百万开发者依赖于我们安全团队的工作。我们将安全视为我们必须在多个时间范围内投资并取得成功的事项，从使今天的模型与我们未来预期的更具能力的系统保持一致。这项工作一直在 OpenAI 中开展，我们的投资将随着时间的推移而增加。

我们信奉一种平衡、科学的方法，其中安全措施从一开始就被整合到开发过程中。这确保了我们的人工智能系统既具有创新性又可靠，并且能够为社会带来好处。

在今天的 AI 首尔峰会上，我们将与行业领袖、政府官员和公民社会成员一起讨论 AI 安全问题。虽然还有更多工作要做，但我们对 OpenAI 和其他公司今天达成的额外前沿 AI 安全承诺感到鼓舞。这些承诺呼吁公司在安全开发和部署前沿 AI 模型的同时，分享有关其风险缓解措施的信息，与我们已经采取的步骤保持一致。其中包括承诺发布安全框架，例如我们去年制定并采纳的“准备框架”。 

我们分享了我们积极使用和改进的 10 项实践。

1. **发布前的经验模型红队测试：**我们在发布前根据我们的“准备框架”和自愿承诺在内部和外部对模型安全性进行经验评估。如果新模型超过了我们“准备框架”中的“中等”风险阈值，我们将不会发布该模型，直到实施足够的安全干预措施将后处理评分恢复到“中等”水平。超过 70 名外部专家通过我们的外部红队测试工作帮助评估了与 GPT-4o 相关的风险，我们利用这些经验构建了基于早期检查点中弱点的评估，以更好地了解后期检查点。
2. **一致性和安全研究：**随着时间的推移，我们的模型变得越来越安全。这归因于构建更智能的模型，这些模型通常会减少事实错误，并且在逆境条件下（如越狱）更不可能输出有害内容。这也归因于我们专注于实际一致性、安全系统和训练后研究的投资。这些努力旨在改善人类生成的微调数据的质量，并且在未来，改善我们的模型被训练遵循的指令。我们还正在进行并发布旨在大幅提高我们系统对越狱等攻击的鲁棒性的基础研究。
3. **滥用监控：**随着我们通过我们的 API 和 ChatGPT 部署越来越强大的语言模型，我们利用了广泛的工具范围，包括专用的审核模型和我们自己的模型来监控安全风险和滥用情况。我们沿途分享了一些关键发现，包括与微软联合披露我们技术被国家行为者滥用的情况，以便其他人可以更好地防范类似风险。我们还使用 GPT-4 进行内容政策制定和内容审核决策，为政策的细化提供更快的反馈循环，并减少暴露给人工审核者的滥用材料。
4. **安全系统化方法：**我们在模型的整个生命周期的每个阶段实施一系列安全措施，从预训练到部署。随着我们在开发更安全、更一致的模型行为方面取得进展，我们还投资于预训练数据安全、系统级模型行为引导、数据飞轮以持续改进安全和健壮的监控基础设施。
5. **保护儿童：**我们安全工作的一个关键重点是保护儿童。我们在 ChatGPT 和 DALL·E 中构建了强大的默认防护栏和安全措施，以减轻对儿童的潜在危害。在 2023 年，我们与 Thorn 的 Safer 合作，以检测、审核和报告用户试图将儿童性虐待材料上传到我们的图像工具时，将其报告给国家儿童失踪和被剥削中心。我们继续与 Thorn、技术联盟、All Tech is Human、Commonsense Media 和更广泛的科技社区合作，以维护安全设计原则。
6. **选举公正：**我们正在与政府和利益相关者合作，以防止滥用，确保 AI 生成内容的透明度，并改善准确选民信息的获取。为此，我们引入了一项用于识别由 DALL·E 3 创建的图像的工具，加入了内容真实性倡议（C2PA）的指导委员会，并在 DALL·E 3 中包含了 C2PA 元数据，以帮助人们了解他们在网上找到的媒体的来源。ChatGPT 现在将用户引导至美国和欧洲的官方选民信息来源。此外，我们支持美国参议院提出的两党“保护选举免受欺诈 AI 法案”，该法案将禁止在政治广告中使用误导性 AI 生成内容。
7. **投资于影响评估和政策分析：**我们的影响评估工作在研究、行业规范和政策方面产生了广泛影响，包括我们早期在测量与 AI 系统相关的化学、生物、放射和核（CBRN）风险方面的工作，以及我们估算不同职业和行业可能受到语言模型影响程度的研究。我们还发布了关于社会如何最好地管理相关风险的开创性工作，例如与外部专家合作评估语言模型对影响运作的影响的工作。
8. **安全和访问控制措施：**我们优先保护我们的客户、知识产权和数据。我们将我们的 AI 模型部署到世界各地作为服务，并通过 API 控制访问，从而实现政策执行。我们的网络安全工作包括根据需要限制对训练环境和高价值算法秘密的访问、内部和外部渗透测试、漏洞赏金计划等。我们相信保护先进的 AI 系统将受益于基础设施安全的演变，正在探索诸如 GPU 的保密计算和将 AI 应用于网络防御的新颖控制措施。为了加强网络防御，我们正在通过我们的网络安全资助计划资助第三方安全研究人员。
9. **与政府合作伙伴关系：**我们与世界各地的政府合作，以制定有效和适应性强的 AI 安全政策。这包括展示我们的工作和分享我们的经验，与政府和其他第三方合作进行试点保证，并参与公众对新标准和法律的辩论。
10. **安全决策和董事会监督：**作为我们“准备框架”的一部分，我们有一个安全决策的运营结构。我们的跨职能安全咨询小组在部署前审核模型能力报告，并在部署前提出建议。公司领导层做出最终决定，董事会对这些决定进行监督。

这种方法使我们能够在当前能力水平上构建和部署安全而有能力的模型。

随着我们迈向下一个前沿模型，我们意识到我们将需要改进我们的实践，特别是提高我们的安全姿态，以最终能够抵御复杂的国家行为者攻击，并确保我们在主要发布之前为安全测试增加额外的时间。我们和这个领域都有一个难题需要解决，以便安全和有益地提供日益强大的人工智能。我们计划在未来几周分享更多关于这些不断发展的实践的信息。

---

- [原文](https://openai.com/index/openai-safety-update/)
- 本文
    <!-- - [博客 - 从零开始学AI](...) -->
    <!-- - [Blog | Learn AI from scratch](...) -->
    <!-- - [公众号 - 从零开始学AI](...) -->
    <!-- - [CSDN - 从零开始学AI](...) -->
    <!-- - [掘金 - 从零开始学AI](...) -->
    <!-- - [知乎 - 从零开始学AI](...) -->
    <!-- - [阿里云 - 从零开始学AI](...) -->
    <!-- - [腾讯云 - 从零开始学AI](...) -->
